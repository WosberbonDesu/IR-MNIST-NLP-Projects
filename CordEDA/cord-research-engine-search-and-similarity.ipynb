{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\n<a href=\"#1.--Installing-the-dgunning/cord19-Library\">1. Installing the cord library</a>\n\n<a href=\"#2.--Loading-Research-Papers\">2. Loading Research Papers</a>\n\n<a href=\"#3.-Searching-Research-Papers\">3. Searching Research Papers</a>\n\n&nbsp;&nbsp;<a href=\"#With-the-search-function\">  With the search function</a>\n\n&nbsp;&nbsp;<a href=\"#With-the-searchbar\">  With the searchbar</a>\n\n<a href=\"#4.-Selecting-Research-Papers\">4. Selecting Research Papers</a>\n\n<a href=\"#5.-Selecting-Individual-Papers\">5. Selecting Individual Papers</a>\n\n<a href=\"#6.-Exploratory-Analysis-and-Charts\">6. Exploratory Analysis and Charts</a>\n\n<a href=\"#7.-Technical-Notes\">7. Technical Notes</a>\n\n&nbsp;&nbsp;<a href=\"#What-is-BM25\">  What is BM25</a>\n\n&nbsp;&nbsp;<a href=\"#Preprocessing-Text\">  Preprocessing Text</a>\n\n&nbsp;&nbsp;<a href=\"#Loading-JSON\">  Loading JSON</a>\n\n&nbsp;&nbsp;<a href=\"#Parallel-Processing-Code\">  Parallel Processing Code</a>\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 1.  Installing the dgunning/cord19 Library\n\nThe code for this kernel is maintained at https://github.com/dgunning/cord19. To install it in the Kaggle kernel, the Internet must be set to ON."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install -U git+https://github.com/dgunning/cord19.git","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Technical Design Notes\nFor current details on the design of the **cord** library, check the project on [github/dgunning/cord19](https://github.com/dgunning/cord19)\n\nThe **ResearchPapers** class is a container for the metadata, and the BM25 search index. It contains functions to find papers using **index** `research_papers[0]`,  **cord_uid** `research_papers[\"4nmc356g\"]`, **OR** to create subsets of ResearchPapers like `papers.since_sarscov2`, **OR** to run `search()` or display the `searchbar()`\n\nBecause the ResearchPapers class is simply a container for the metadata dataframe, and all useful information about each paper is on the dataframe as a column, including the **index_tokens**, tags such as **covid_related** etc, subsetting ResearchPapers is simply a matter of subsetting the **metadata** dataframe, then creating a new ResearchPapers instance. To create a ResearchPapers instance after a date means \n```{python}\n    def after(self, date, include_null_dates=False):\n        cond = self.metadata.published >= date\n        if include_null_dates:\n            cond = cond | self.metadata.published.isnull()\n        return self._make_copy(self.metadata[cond])\n```\nThus, we implement functions such as **head**, **tail**, **sample**, **query**, which just delegate to the metadata dataframe function and then create a new ResearchPapers instance.\n\n## What happens in ResearchPapers.load?\n1. **load_metadata** - Load the **metadata.csv** file\n```python\n@staticmethod\n    def load_metadata(data_path=None):\n        if not data_path:\n            data_path = find_data_dir()\n\n        print('Loading metadata from', data_path)\n        metadata_path = PurePath(data_path) / 'metadata.csv'\n        dtypes = {'Microsoft Academic Paper ID': 'str', 'pubmed_id': str}\n        renames = {'source_x': 'source', 'has_full_text': 'has_text'}\n        metadata = pd.read_csv(metadata_path, dtype=dtypes, low_memory=False,\n                               parse_dates=['publish_time']).rename(columns=renames)\n```\n2. **clean_metadata** - Clean the metadata\n```python\ndef clean_metadata(metadata):\n    print('Cleaning metadata')\n    return metadata.pipe(start) \\\n        .pipe(clean_title) \\\n        .pipe(clean_abstract) \\\n        .pipe(rename_publish_time) \\\n        .pipe(add_date_diff) \\\n        .pipe(drop_missing) \\\n        .pipe(fill_nulls) \\\n        .pipe(apply_tags)\n```\n\n3. **Create the BM25 Search Index**\n\n**ResearchPapers** can be indexed with the metadata *abstracts* OR with the *text* content of the paper. Indexing from the abstracts is straightforward - we just apply a **preprocess** function to clean and tokenize the abstract. Indexing from the texts - if no json-cache exists - happens by loading the JSON files and, tokenizing the texts and setting the **index_tokens** on the metadata. However, there is now a **json_cache** dataset comprised of the preprocessed text tokens, along with the JSOn file's sha - which we use to merge into the metadata.\n\nAfter the metadata is loaded and cleaned we create the **BM25** index inside of **ResearchPapers.__init__()**\n\n```python\n      \nif 'index_tokens' not in metadata:\n    print('\\nIndexing research papers')\n    if any([index == t for t in ['text', 'texts', 'content', 'contents']]):\n        _set_index_from_text(self.metadata, data_dir)\n    else:\n        print('Creating the BM25 index from the abstracts of the papers')\n        print('Use index=\"text\" if you want to index the texts of the paper instead')\n        tick = time.time()\n        self.metadata['index_tokens'] = metadata.abstract.apply(preprocess)\n        tock = time.time()\n        print('Finished Indexing in', round(tock - tick, 0), 'seconds')\n\n```\n\n## Creating Document Vectors\nCreating document vectors is simple. First we load the cached JSON tokens. (This is a cached version of the JSON files optimized for memory and disk space)\n\n```python\njson_tokens = []\nfor catalog in JSON_CATALOGS:\n    json_cache = load_json_cache(catalog)\n    json_tokens.append(json_cache)\n    \njson_tokens = pd.concat(json_tokens, ignore_index=True)\n```\n\nthen we train a **gensim Doc2Vec** model to create vectors with length **VECTOR_SIZE** (currently 20)\n\n```python\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(json_tokens.index_tokens)]\nmodel = Doc2Vec(documents, vector_size=VECTOR_SIZE, window=2, min_count=1, workers=8)\n```\nNext we can create a document vector for each json record\n\n```python\njson_tokens['document_vector'] = json_tokens.index_tokens.apply(model.infer_vector)\n```\n\n### Document Similarity Index\nThe document similarity index is based on [Annoy](https://github.com/spotify/annoy). Annoy is very simple and super fast, and will return the mst similar items to a given query.\n\n```python\nfrom annoy import AnnoyIndex\n\nannoy_index = AnnoyIndex(DOCUMENT_VECTOR_LENGTH, 'angular')  \nfor i in range(len(metadata)):\n    v = json_tokens.loc[i].document_vector\n    annoy_index.add_item(i, v)\n\nannoy_index.build(10) # 10 trees\n```"},{"metadata":{},"cell_type":"markdown","source":"# 2. Loading Research Papers"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from cord import ResearchPapers\n\nresearch_papers = ResearchPapers.load()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Searching Research Papers"},{"metadata":{},"cell_type":"markdown","source":"The ResearchPapers instance provides two main ways of searching - using the function **search()** or using the function **searchbar()** which will show an interactive search bar. \n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"### With the search function\n\nThe ResearchPapers class has a function **search** in which you can specify search terms. The function will use the **BM25** index to retrieve the documents that most closely satisfy the search"},{"metadata":{"trusted":true},"cell_type":"code","source":"research_papers.search('antiviral treatment')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### With the searchbar\nYou can call searchbar() without any arguments to show the widget without any initial search results, or with an initial search string, which we will do below."},{"metadata":{"trusted":true},"cell_type":"code","source":"research_papers.searchbar('antiviral treatment')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Selecting Research Papers\n\nThere are many ways to select subsets of research papers including\n\n- **Papers since SARS**  `research_papers.since_sars()`\n- **Papers since SARS-COV-2** `research_papers.since_sarscov2()`\n- **Papers before SARS** `research_papers.before_sars()`\n- **Papers before SARS-COV-2** `research_papers.before_sarscov2()`\n- **Papers before a date** `research_papers.before('1989-09-12')`\n- **Papers after a date** `research_papers.after('1989-09-12')`\n- **Papers that contains a string** \n- **Papers that match a string (using regex)** \n\nHere we are interested in research papers since the sars-cov-2 outbreak"},{"metadata":{},"cell_type":"markdown","source":"### Research papers since SARS-COV-2"},{"metadata":{"trusted":true},"cell_type":"code","source":"since_covid = research_papers.since_sarscov2()\nsince_covid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Research Papers that match the H{num}N{num} pattern\nThe code below returns papers that match the regex \"H[0-9]N[0-9]\""},{"metadata":{"trusted":true},"cell_type":"code","source":"research_papers.match('H[0-9]N[0-9]')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Match string columns\nTo select research papers that match on a string column, use `research_papers.match(<searchstring>)`. If no column is specified, then the abstract is used.\n\nThe following shows what has been published by **Anthony Fauci** since the SARS-COV-2 outbreak"},{"metadata":{"trusted":true},"cell_type":"code","source":"research_papers.contains(\"Fauci\", column='authors').since_sarscov2()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Selecting Individual Papers\nIndividual papers can be selected from the ResearchPapers instance using the **[]** selectors, using the index number of the paper. Note that you will likely never know the index value of the paper that you need, but this ability will come in handy when using the search tool."},{"metadata":{},"cell_type":"markdown","source":"### Selecting a paper by index\nYou can select any paper in the **ResearchPapers** instance using the index. This will locate and create a new **Paper** instance, which you can output to the notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"paper = research_papers[197]\npaper","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selecting a paper by cord_uid\nYou can also select a paper using its **cord_uid**."},{"metadata":{"trusted":true},"cell_type":"code","source":"paper = research_papers['asf5c7xu'] # or research_papers['5c31897d01f3edc7f58a0f03fceec2373fcfdc3d']\npaper","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This would be similar to querying research_papers for a single sha, but in this case a **ResearchPapers** instance is returned."},{"metadata":{"trusted":true},"cell_type":"code","source":"research_papers.query(\"sha=='5c31897d01f3edc7f58a0f03fceec2373fcfdc3d'\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Viewing a Research Paper\nOnce you have a research paper, there are many ways to view it.\n\n- **Overview** A nicely formatted view of the paper's important fields\n- **Abstract** The paper's abstract\n- **Summary** A summary of the paper's abstract using the **TextRank** algorithm\n- **Text** The text the paper\n- **HTML** The contents of the paper as somewhat nicely formatted HTML\n- **Text Summary** The text of the paper, summarized using the **TextRank** algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from ipywidgets import interact\nfrom IPython.display import display\n\npaper = research_papers['asf5c7xu']\n\ndef view_paper(ViewPaperAs):\n    if ViewPaperAs == 'Overview':\n        display(paper)\n    elif ViewPaperAs == 'Abstract':\n        display(paper.abstract)\n    elif ViewPaperAs == 'Summary of Abstract':\n        display(paper.summary)\n    elif ViewPaperAs == 'HTML':\n        display(paper.html)\n    elif ViewPaperAs == 'Text':\n        display(paper.text)\n    elif ViewPaperAs == 'Summary of Text':\n        display(paper.text_summary)\n    \ninteract(view_paper,\n         ViewPaperAs=['Overview', # Show an overview of the paper's important fields and statistics\n                      'Abstract', # Show the paper's abstract\n                      'Summary of Abstract', # Show a summary of the paper's abstract\n                      'HTML', # Show the paper's contents as (slightly) formatted HTML\n                      'Text', # Show the paper's contents\n                      'Summary of Text' # Show a summary of the paper's content\n                     ]\n        );","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding Similar Papers"},{"metadata":{"trusted":true},"cell_type":"code","source":"research_papers.similar_to('asf5c7xu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Widgets in Jupyter notebook\nThe code above also show how to use **ipywidgets interact** and **IPython display** to provide a userinterface inside a Jupyter notebook. "},{"metadata":{},"cell_type":"markdown","source":"# 6. Exploratory Analysis and Charts"},{"metadata":{},"cell_type":"markdown","source":"### Load Document Vectors\n\nEach research paper's content was converted to a document vector using **gensim's Doc2Vec**. We load the document vectors and filter out any empty vectors. (We have some empty vectors where the research paper was not associated with any JSON content)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport ipywidgets as widgets\nfrom cord.core import cord_support_dir\n\n# Load the document vectors\nvectors = pd.read_parquet(cord_support_dir()/ 'DocumentVectors.pq').reset_index()\ndisplay(widgets.HTML('<h4>Document Vectors</h4>'))\ndisplay(vectors.head())\n\n# Use the metadata of the research_papers\nmetadata = research_papers.metadata.copy()\n\nvector_metadata_merge = vectors.merge(metadata, on='cord_uid', how='right')\nvectors['covid_related'] = vector_metadata_merge.covid_related\nvectors['published'] = vector_metadata_merge.published\ndisplay(widgets.HTML('<h4>Document Vectors with covid_related and published</h4>'))\ndisplay(vectors.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Chart showing clusters of research papers\nThe chart below shows the research papers in 2D space. The chart below is done in **Altair** - the best Python charting library."},{"metadata":{"trusted":true},"cell_type":"code","source":"import altair as alt\n\n\nvector_df = pd.DataFrame({'x':vectors.x,\n                          'y': vectors.y,\n                          '1d': vectors['1d'],\n                          'cluster': vectors.cluster,\n                          'covid_related': vectors.covid_related,\n                          'published': vectors.published})  # Ensure 5000 limit\n\nalt.Chart(vector_df.sample(5000)).mark_point().encode(\n       x=alt.X('x', axis=None),\n       y=alt.Y('y', axis=None),\n       color= 'cluster:N'\n    ).properties(\n        title='CORD Research Papers in 2D space',\n        width=600,\n        height=400\n    ).configure_axis(\n        grid=False\n    ).configure_view(\n        strokeWidth=0\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identify which clusters are covid related"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.style as style\nstyle.use('fivethirtyeight')\ncovid_count = vector_df[['cluster', 'covid_related']].groupby(['cluster']).sum() \ncluster_count = vector_df[['cluster', 'covid_related']].groupby(['cluster']).count() \ncovid_cluster_stats = (covid_count / cluster_count) * 100\ncovid_cluster_stats = covid_cluster_stats.sort_values(['covid_related'])\nfig = covid_cluster_stats.plot.barh(grid=False, figsize=(6, 3), legend=False, title='% Covid Related');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Research Papers since COVID\n\nWe plot the 1d compression of the 768-dimension document vector over time, with the clusters identified by color. Then we plot clusters 2 and 6. We see a big increase in the number of papers since COVID appeared"},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_since = vector_df.query('published > \"2015-01-01\" & (cluster==2 | cluster==6)').copy()\nvector_since.loc[vector_since.published> '2020-06-30', 'published'] = pd.to_datetime('2020-03-30')\nif len(vector_since) > 5000:\n    vector_since = vector_since.sample(5000)\nalt.Chart(vector_since).mark_point().encode(\n       x=alt.X('published:T'),\n       y=alt.Y('1d'),\n       color= 'cluster:N'\n    ).properties(\n        title='CORD Research Papers since 2015',\n        width=600,\n        height=400\n    ).configure_axis(\n        grid=False\n    ).configure_view(\n        strokeWidth=0\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### New Feature - 2D Visual Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"query =\"\"\"\nEfforts to identify the underlying drivers of fear, anxiety and stigma that\nfuel misinformation and rumor, particularly through social media.\n\"\"\"\nresearch_papers.search_2d(query)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Technical Notes\nWhat follows here will be a discussion of the technical design of the CORD library with code snippets"},{"metadata":{},"cell_type":"markdown","source":"## What is BM25\n\n**BM25** stands for **Best Match the 25th iteration** and is a text search algorithm first developed in 1994. It is one of the best search algorithms available, and **Lucene**, and its derivatives **Solr** and **ElasticSearch** switched to a **BM25** variant around 2015."},{"metadata":{},"cell_type":"markdown","source":"### Creating a simple BM25 index\nTo show how to create a BM25 index, we will first load the metadata, and just use a subset of the columns and the rows"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom rank_bm25 import BM25Okapi\npd.options.display.max_colwidth=160\n\nmeta_df = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv') # Or pd.read_csv()\nmeta_df = meta_df[['title', 'abstract', 'publish_time']].head(1000)\nmeta_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing Text\nTo create the index, we need a list of the tokens in each string. The easiest way to do so is by using **gensim** or **NLTK** to tokenize each abstract into a list of tokens. Here we use **gensim**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import preprocess_documents, preprocess_string\n\nmeta_df_tokens = meta_df.abstract.fillna('').apply(preprocess_string)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we create a **BM25Okapi** index from the tokens. We also implement a search function that returns the top 10 results from the search.\nNote that in search wer are asking the index to return the dataframe indexes of the tokens most similar to the search string."},{"metadata":{"trusted":true},"cell_type":"code","source":"from rank_bm25 import BM25Okapi\nimport numpy as np\n\nbm25_index = BM25Okapi(meta_df_tokens.tolist())\n\ndef search(search_string, num_results=10):\n    search_tokens = preprocess_string(search_string)\n    scores = bm25_index.get_scores(search_tokens)\n    top_indexes = np.argsort(scores)[::-1][:num_results]\n    return top_indexes\n\nindexes = search('novel coronavirus treatment')\nindexes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can use the indexes to locate the rows in **meta_df** which best match the search terms"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.loc[indexes, ['abstract', 'publish_time']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Or use the search function directly in **meta_df.loc[]**"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.loc[search('novel coronavirus treatment')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parallel Processing Code\nCreating the BM25 index from the contents of over 44000 research papers requires running the preprocessing and tokenization in parallel. The following code includes a function called **parallel** which applies a function over a Collection. The parallel function is a modification of a function by the same name from the **fastai v2 repository**, with the main changes being using **tqdm** progress bars, and ensuring the output list is sorted in the same order as the input list. \n\nOnce the **parallel** function is defined it can be used as follows `papers = parallel(load_json, list(json_catalog_path.glob('*.json')))`\n\nClick **Code** to view"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import multiprocessing\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import Collection, Any\n\ndef is_notebook():\n    try:\n        from IPython import get_ipython\n        return get_ipython().__class__.__name__ == \"ZMQInteractiveShell\"\n    except (NameError, ImportError):\n        return False\n\nif is_notebook():\n    from tqdm.notebook import tqdm\nelse:\n    from tqdm import tqdm\n    \ndef ifnone(a: Any, b: Any) -> Any:\n    return b if a is None else a\n\ndef parallel(func, arr: Collection, max_workers: int = None):\n    \"Call `func` on every element of `arr` in parallel using `max_workers`.\"\n    max_workers = ifnone(max_workers, multiprocessing.cpu_count())\n    progress_bar = tqdm(arr)\n    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n        futures_to_index = {ex.submit(func, o): i for i, o in enumerate(arr)}\n        results = []\n        for f in as_completed(futures_to_index):\n            results.append((futures_to_index[f], f.result()))\n            progress_bar.update()\n        for n in range(progress_bar.n, progress_bar.total):\n            time.sleep(0.1)\n            progress_bar.update()\n        results.sort(key=lambda x: x[0])\n    return [result for i, result in results]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading JSON\n\nHere are some of the functions for loading JSON files\n\nClick **Code** to view"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from functools import partial\n\ndef get_text(paper_json, text_key) -> str:\n    \"\"\"\n    :param paper_json: The json\n    :param text_key: the text_key - \"body_text\" or \"abstract\"\n    :return: a text string with the sections\n    \"\"\"\n    body_dict = collections.defaultdict(list)\n    for rec in paper_json[text_key]:\n        body_dict[rec['section']].append(rec['text'])\n\n    body = ''\n    for section, text_sections in body_dict.items():\n        body += section + '\\n\\n'\n        for text in text_sections:\n            body += text + '\\n\\n'\n    return body\n\n\nget_body = partial(get_text, text_key='body_text')\nget_abstract = partial(get_text, text_key='abstract')\n\ndef author_name(author_json):\n    first = author_json.get('first')\n    middle = \"\".join(author_json.get('middle'))\n    last = author_json.get('last')\n    if middle:\n        return ' '.join([first, middle, last])\n    return ' '.join([first, last])\n\n\ndef get_affiliation(author_json):\n    affiliation = author_json['affiliation']\n    institution = affiliation.get('institution', '')\n    location = affiliation.get('location')\n    if location:\n        location = ' '.join(location.values())\n    return f'{institution}, {location}'\n\n\ndef get_authors(paper_json, include_affiliation=False):\n    if include_affiliation:\n        return [f'{author_name(a)}, {get_affiliation(a)}'\n                for a in paper_json['metadata']['authors']]\n    else:\n        return [author_name(a) for a in paper_json['metadata']['authors']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Example of displaying Research Papers on Information Sharing\nThe CORD research tool can be use to find papers specific for the tasks in this dataset. Here is an example"},{"metadata":{"trusted":true},"cell_type":"code","source":"research_papers.display('dao10kx9', 'rjc3b4br',  'r0lduvs1', '7i422cht', 'pa9h6d0a', 'dbzrd23n', '5gbkrs73', '94tdt2rv', \n                        'xsgxd5sy', 'jf36as70', 'uz91cd6h')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A more detailed notebook on **Information Sharing** is here https://www.kaggle.com/dgunning/cord-research-on-information-sharing"},{"metadata":{},"cell_type":"markdown","source":"## Implementing the Specter Vector API\n\nThe AllenAI team has provided a public API for their Specter Vectors, which allows you to get the 768 dimension representation of a paper's title, abstract or text. Here is one implementation.\n\nFollow the official guide here\nhttps://github.com/allenai/paper-embedding-public-apis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Dict, List\nimport requests\n\nSPECTER_URL = \"https://model-apis.semanticscholar.org/specter/v1/invoke\"\nMAX_BATCH_SIZE = 16\n\ndef chunks(lst, chunk_size=MAX_BATCH_SIZE):\n    \"\"\"Splits a longer list to respect batch size\"\"\"\n    for i in range(0, len(lst), chunk_size):\n        yield lst[i: i + chunk_size]\n\n\ndef get_embeddings_for_papers(papers: List[Dict[str, str]]):\n    embeddings_by_paper_id: Dict[str, List[float]] = {}\n    for chunk in chunks(papers):\n        # Allow Python requests to convert the data above to JSON\n        response = requests.post(SPECTER_URL, json=chunk)\n\n        if response.status_code != 200:\n            print(\"Something went wrong on the spector API side .. try again\")\n            return None\n\n        for paper in response.json()[\"preds\"]:\n            embeddings_by_paper_id[paper[\"paper_id\"]] = paper[\"embedding\"]\n\n    return embeddings_by_paper_id\n\ndef get_embeddings(title: str, abstract: str = None):\n    abstract = abstract or title\n    paper = {\"paper_id\": \"paper\", \"title\": title, \"abstract\": abstract}\n    embeddings = get_embeddings_for_papers([paper])\n    return embeddings['paper'] if embeddings else None\n\ndef plot_embeddings(vector):\n    df = pd.DataFrame(vector)\n    ax = df.plot.bar(figsize=(10,1))\n    ax.get_legend().remove()\n    ax.axes.set_xticklabels([])\n    ax.axes.set_yticklabels([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get the Embeddings\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings = get_embeddings('Animal to human transmission')\n\nembeddings[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot the embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_embeddings(embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot multiple embeddings\n\nIf we plot multiple embeddings together we can see the similarity of the vectors for similar queries"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_embeddings(get_embeddings('Animal to human viral transmission'))\nplot_embeddings(get_embeddings('Bat to human viral transmission'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}